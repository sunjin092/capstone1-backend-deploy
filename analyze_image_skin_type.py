import torch
import os
import numpy as np
from torchvision import models, transforms
from PIL import Image, ImageOps
import torch.nn as nn
import cv2
import mediapipe as mp
import io
import math

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… ìƒëŒ€ê²½ë¡œë¡œ ìˆ˜ì •
regression_ckpt = os.path.join("checkpoint", "regression")
regression_num_output = [1, 2, 0, 0, 0, 3, 3, 0, 2]

# ì´ë¯¸ì§€ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# ê²°ê³¼ ë³µì› ê¸°ì¤€
restore_stats = {
    1: {"ìˆ˜ë¶„": (60.6, 10.1), "íƒ„ë ¥": (48.7, 11.9)},
    5: {"ìˆ˜ë¶„": (60.6, 10.1), "íƒ„ë ¥": (48.7, 11.9), "ëª¨ê³µ ê°œìˆ˜": "log"},
    6: {"ìˆ˜ë¶„": (60.2, 9.6), "íƒ„ë ¥": (49.3, 12.1), "ëª¨ê³µ ê°œìˆ˜": "log"},
    8: {"ìˆ˜ë¶„": (61.3, 10.0), "íƒ„ë ¥": (47.5, 12.0)},
    0: {"ìƒ‰ì†Œì¹¨ì°© ê°œìˆ˜": 300}
}
area_label = {
    0: "ì „ì²´", 1: "ì´ë§ˆ", 2: "ë¯¸ê°„", 3: "ì™¼ìª½ ëˆˆê°€", 4: "ì˜¤ë¥¸ìª½ ëˆˆê°€",
    5: "ì™¼ìª½ ë³¼", 6: "ì˜¤ë¥¸ìª½ ë³¼", 7: "ì…ìˆ ", 8: "í„±"
}
reg_desc = {
    0: ["ìƒ‰ì†Œì¹¨ì°© ê°œìˆ˜"],
    1: ["ìˆ˜ë¶„", "íƒ„ë ¥"],
    5: ["ìˆ˜ë¶„", "íƒ„ë ¥", "ëª¨ê³µ ê°œìˆ˜"],
    6: ["ìˆ˜ë¶„", "íƒ„ë ¥", "ëª¨ê³µ ê°œìˆ˜"],
    8: ["ìˆ˜ë¶„", "íƒ„ë ¥"]
}

# Mediapipe ì–¼êµ´ ì˜ì—­ crop
mp_face_mesh = mp.solutions.face_mesh
REGION_LANDMARKS = {
    0: list(range(468)),
    1: [10, 67, 69, 71, 109, 151, 337, 338, 297],
    2: [168, 6, 197, 195, 5, 4],
    3: [130, 133, 160, 159, 158],
    4: [359, 362, 386, 385, 384],
    5: [205, 50, 187, 201, 213],
    6: [425, 280, 411, 427, 434],
    7: [13, 14, 17, 84, 181],
    8: [152, 377, 400, 378, 379]
}

def crop_regions_by_ratio(pil_img, visualize=False):
    img = np.array(pil_img)
    img_rgb = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    h, w = img.shape[:2]
    regions = [None] * 9
    with mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True) as face_mesh:
        results = face_mesh.process(img_rgb)
        if not results.multi_face_landmarks:
            raise ValueError("â— ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        landmarks = results.multi_face_landmarks[0].landmark
        points = np.array([(lm.x * w, lm.y * h) for lm in landmarks])
        face_x1, face_y1 = np.min(points, axis=0)
        face_x2, face_y2 = np.max(points, axis=0)
        face_w, face_h = face_x2 - face_x1, face_y2 - face_y1
        for idx, lm_indices in REGION_LANDMARKS.items():
            pts = np.array([(landmarks[i].x * w, landmarks[i].y * h) for i in lm_indices])
            cx, cy = np.mean(pts, axis=0)
            if idx == 8: cx -= face_w * 0.15
            if idx == 1:
                box_w, box_h = int(face_w * 0.70), int(face_h * 0.3)
                cy -= box_h * 0.2
            elif idx == 2:
                box_w, box_h = int(face_w * 0.35), int(face_h * 0.15)
                cy -= box_h * 2.5
            else:
                box_w, box_h = int(face_w * 0.28), int(face_h * 0.25)
            x1, y1 = max(int(cx - box_w / 2), 0), max(int(cy - box_h / 2), 0)
            x2, y2 = min(int(cx + box_w / 2), w), min(int(cy + box_h / 2), h)
            crop = img[y1:y2, x1:x2]
            regions[idx] = Image.fromarray(crop)
    return regions

# íšŒê·€ ëª¨ë¸ë§Œ ë¶ˆëŸ¬ì˜¤ê¸°
reg_models = [None] * 9
for idx in [0, 1, 5, 6, 8]:
    out_dim = regression_num_output[idx]
    model = models.resnet50(weights=None)
    model.fc = nn.Linear(model.fc.in_features, out_dim)
    ckpt_path = os.path.join(regression_ckpt, str(idx), "state_dict.bin")
    if os.path.exists(ckpt_path):
        state = torch.load(ckpt_path, map_location=device)
        if "model_state" in state:
            state = state["model_state"]
        model.load_state_dict(state, strict=False)
        model.eval()
        reg_models[idx] = model.to(device)

# âœ… ë¶„ì„ í•¨ìˆ˜

def run_analysis(image_bytes):
    result = {}
    image = Image.open(io.BytesIO(image_bytes)).convert("RGB")
    image = ImageOps.exif_transpose(image)
    try:
        regions = crop_regions_by_ratio(image, visualize=False)
    except Exception as e:
        return {"error": f"ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨: {str(e)}"}

    region_results = {}
    for idx in [0, 1, 5, 6, 8]:
        print(f"ğŸ” ì˜ì—­ {idx} ë¶„ì„ ì¤‘...")
        if reg_models[idx] is None:
            print(f"âš ï¸  reg_model[{idx}] is None â†’ SKIP")
            continue
        if regions[idx] is None:
            print(f"âš ï¸  regions[{idx}] is None â†’ SKIP")
            continue

        crop_tensor = transform(regions[idx]).unsqueeze(0).to(device)
        with torch.no_grad():
            reg_out = reg_models[idx](crop_tensor).squeeze().cpu().numpy()
        if reg_out.ndim == 0:
            reg_out = [reg_out]

        area_name = area_label[idx]
        values = {}
        for i, val in enumerate(reg_out):
            label = reg_desc[idx][i]
            if label == "ëª¨ê³µ ê°œìˆ˜" and restore_stats[idx].get(label) == "log":
                val = np.clip(np.exp(val) - 1, 0, 2500)
            elif label in restore_stats[idx] and isinstance(restore_stats[idx][label], tuple):
                mean, std = restore_stats[idx][label]
                val = val * std + mean
            elif label == "ìƒ‰ì†Œì¹¨ì°© ê°œìˆ˜":
                val *= 300
            values[label] = round(float(val), 2)
        region_results[area_name] = values
        print(f"âœ… {area_name} ê²°ê³¼: {values}")

    print("ğŸŸ¢ ì „ì²´ ê²°ê³¼:", region_results)
    result["regions"] = region_results

    # âœ… JSON ì§ë ¬í™” ê°€ëŠ¥í•œ ê°’ìœ¼ë¡œ ì •ë¦¬
    for region in result["regions"].values():
        for k, v in region.items():
            if not math.isfinite(v):  # nan, inf, -inf ë°©ì§€
                region[k] = 0

    return result
